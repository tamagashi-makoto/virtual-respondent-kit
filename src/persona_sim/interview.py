"""ãƒ‡ãƒ—ã‚¹ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€‚

AIã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ã‚¢ãƒ¼ãŒãƒšãƒ«ã‚½ãƒŠã«å¯¾ã—ã¦æ·±æ˜ã‚Šè³ªå•ã‚’è¡Œã„ã€å®šæ€§çš„ãªã‚¤ãƒ³ã‚µã‚¤ãƒˆã‚’ç™ºæ˜ã™ã‚‹ã€‚
LangGraphã‚’ä½¿ç”¨ã—ãŸãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå¯¾è©±ã‚·ã‚¹ãƒ†ãƒ ã€‚
"""
import asyncio
import json
import os
from pathlib import Path
from typing import Annotated, List, Optional

import pandas as pd
from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage
from langgraph.graph import END, START, StateGraph
from tqdm.asyncio import tqdm
from typing_extensions import TypedDict
import operator

from .config import load_config
from .llm import create_llm
from .prompts import get_interviewer_system_prompt, get_persona_system_prompt


class InterviewState(TypedDict):
    """ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã®çŠ¶æ…‹ã€‚"""

    messages: Annotated[List[BaseMessage], operator.add]  # è¿½è¨˜å°‚ç”¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´
    persona_profile: dict
    turn_count: int


class InterviewRunner:
    """ãƒ‡ãƒ—ã‚¹ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼å®Ÿè¡Œã‚¯ãƒ©ã‚¹ã€‚

    AIã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ã‚¢ãƒ¼ã¨ãƒšãƒ«ã‚½ãƒŠã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å¯¾è©±ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã™ã‚‹ã€‚
    """

    def __init__(self, config: Optional[dict] = None):
        """åˆæœŸåŒ–ã€‚

        Args:
            config: è¨­å®šè¾æ›¸ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰ã€‚æŒ‡å®šã—ãªã„å ´åˆã¯config.yamlã‹ã‚‰èª­ã¿è¾¼ã‚€ã€‚
        """
        self.config = config or load_config()

        # LLMåˆæœŸåŒ–
        self.llm = create_llm(self.config)

        # è¨­å®šå€¤
        self.input_file = self.config["interview"]["input_file"]
        self.output_file = self.config["interview"]["output_file"]
        self.max_turns = self.config["interview"]["max_turns"]
        self.concurrent_limit = self.config["interview"]["concurrent_limit"]
        self.initial_question = self.config["interview"]["initial_question"]

        # LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼æ§‹ç¯‰
        self.app = self._build_workflow()

    def _build_workflow(self) -> "CompiledStateGraph":
        """LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚

        Returns:
            CompiledStateGraph: ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã•ã‚ŒãŸãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼
        """
        workflow = StateGraph(InterviewState)

        workflow.add_node("interviewer", self._interviewer_node)
        workflow.add_node("persona", self._persona_node)

        workflow.add_edge(START, "persona")

        def should_continue(state: InterviewState) -> str:
            """ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã‚’ç¶šã‘ã‚‹ã‹åˆ¤å®šã™ã‚‹ã€‚

            Args:
                state: ç¾åœ¨ã®çŠ¶æ…‹

            Returns:
                str: æ¬¡ã®ãƒãƒ¼ãƒ‰åï¼ˆ"interviewer" or ENDï¼‰
            """
            if state["turn_count"] >= self.max_turns:
                return END
            return "interviewer"

        workflow.add_conditional_edges("persona", should_continue)
        workflow.add_edge("interviewer", "persona")

        return workflow.compile()

    async def _persona_node(self, state: InterviewState) -> dict:
        """ãƒšãƒ«ã‚½ãƒŠãƒãƒ¼ãƒ‰ã€‚

        Args:
            state: ç¾åœ¨ã®çŠ¶æ…‹

        Returns:
            dict: æ›´æ–°ã•ã‚ŒãŸçŠ¶æ…‹
        """
        profile = state["persona_profile"]

        system_prompt = f"""You are a real Japanese person with the following profile.

## Your Profile
- Age: {profile.get("age")} / Sex: {profile.get("sex")}
- Occupation: {profile.get("occupation")}
- Region: {profile.get("prefecture")}

## Detailed Persona & Values
- Personality: {profile.get("persona")}
- Professional Stance: {profile.get("professional_persona")}
- Hobbies: {profile.get("hobbies_and_interests")}

Please answer the interviewer's questions acting fully as this person.
Speak your "honest feelings" and "concerns" based on your daily life reality, not just shallow polite answers.
"""

        messages = [SystemMessage(content=system_prompt)] + state["messages"]
        response = await self.llm.ainvoke(messages)

        return {"messages": [response], "turn_count": 0}

    async def _interviewer_node(self, state: InterviewState) -> dict:
        """ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ã‚¢ãƒ¼ãƒãƒ¼ãƒ‰ã€‚

        Args:
            state: ç¾åœ¨ã®çŠ¶æ…‹

        Returns:
            dict: æ›´æ–°ã•ã‚ŒãŸçŠ¶æ…‹
        """
        last_answer = state["messages"][-1].content

        system_prompt = get_interviewer_system_prompt()

        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=f"Respondent's Answer: {last_answer}\n\nCreate ONE deep-dive question for this."),
        ]

        response = await self.llm.ainvoke(messages)

        return {"messages": [response], "turn_count": 1}

    async def _run_single_interview(self, persona: dict, semaphore: asyncio.Semaphore) -> Optional[dict]:
        """å˜ä¸€ãƒšãƒ«ã‚½ãƒŠã«å¯¾ã—ã¦ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã‚’å®Ÿè¡Œã™ã‚‹ã€‚

        Args:
            persona: ãƒšãƒ«ã‚½ãƒŠãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«
            semaphore: ä¸¦åˆ—å®Ÿè¡Œåˆ¶å¾¡ç”¨ã‚»ãƒãƒ•ã‚©

        Returns:
            Optional[dict]: ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼çµæœ
        """
        async with semaphore:
            try:
                initial_state = {
                    "messages": [HumanMessage(content=self.initial_question)],
                    "persona_profile": persona,
                    "turn_count": 0,
                }

                final_state = await self.app.ainvoke(initial_state)

                # ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆ
                transcript = self._create_transcript(final_state["messages"])
                final_answer = final_state["messages"][-1].content

                return {
                    "ID": persona.get("uuid"),
                    "Occupation": persona.get("occupation"),
                    "Age": persona.get("age"),
                    "Conversation_Log": transcript,
                    "Final_Answer": final_answer,
                }

            except Exception as e:
                print(f"Error processing {persona.get("uuid")}: {e}")
                return None

    def _create_transcript(self, messages: List[BaseMessage]) -> str:
        """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã‹ã‚‰ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆã™ã‚‹ã€‚

        Args:
            messages: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆ

        Returns:
            str: ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
        """
        transcript = ""

        for idx, msg in enumerate(messages):
            if idx == 0:
                role = "ã€Initial Questionã€‘"
            elif idx % 2 != 0:
                role = "ã€Persona Answerã€‘"
            else:
                role = "ã€Interviewer Questionã€‘"

            transcript += f"{role}\n{msg.content}\n\n"

        return transcript

    async def run_async(
        self,
        input_file: Optional[str] = None,
        output_file: Optional[str] = None,
        max_turns: Optional[int] = None,
        concurrent_limit: Optional[int] = None,
        initial_question: Optional[str] = None,
    ) -> pd.DataFrame:
        """å…¨ãƒšãƒ«ã‚½ãƒŠã«å¯¾ã—ã¦ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã‚’éåŒæœŸã§å®Ÿè¡Œã™ã‚‹ã€‚

        Args:
            input_file: å…¥åŠ›JSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            output_file: å‡ºåŠ›CSVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            max_turns: æœ€å¤§ã‚¿ãƒ¼ãƒ³æ•°ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            concurrent_limit: ä¸¦åˆ—å®Ÿè¡Œæ•°ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            initial_question: åˆæœŸè³ªå•ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

        Returns:
            pd.DataFrame: ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼çµæœ
        """
        # ãƒ‘ã‚¹è¨­å®š
        input_file = input_file or self.input_file
        output_file = output_file or self.output_file
        max_turns = max_turns or self.max_turns
        concurrent_limit = concurrent_limit or self.concurrent_limit
        initial_question = initial_question or self.initial_question

        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        os.makedirs(os.path.dirname(output_file), exist_ok=True)

        # ãƒšãƒ«ã‚½ãƒŠãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        from .data import load_personas

        personas = load_personas(input_file)

        print(f"ğŸš€ LangGraph Interview Start: {len(personas)} people (Concurrent: {concurrent_limit})")

        semaphore = asyncio.Semaphore(concurrent_limit)

        tasks = [self._run_single_interview(p, semaphore) for p in personas]

        results = []
        for f in tqdm(asyncio.as_completed(tasks), total=len(tasks)):
            res = await f
            if res:
                results.append(res)

        # çµæœã®ä¿å­˜
        df = pd.DataFrame(results)
        df.to_csv(output_file, index=False, encoding="utf-8-sig")
        print(f"\nâœ… Interview Completed. Saved to '{output_file}'.")

        if not df.empty:
            print("\n=== Sample Log (Top 1) ===")
            print(df.iloc[0]["Conversation_Log"][:1000] + "...")

        return df

    def run(
        self,
        input_file: Optional[str] = None,
        output_file: Optional[str] = None,
        max_turns: Optional[int] = None,
        concurrent_limit: Optional[int] = None,
        initial_question: Optional[str] = None,
    ) -> pd.DataFrame:
        """å…¨ãƒšãƒ«ã‚½ãƒŠã«å¯¾ã—ã¦ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã‚’å®Ÿè¡Œã™ã‚‹ã€‚

        Args:
            input_file: å…¥åŠ›JSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            output_file: å‡ºåŠ›CSVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            max_turns: æœ€å¤§ã‚¿ãƒ¼ãƒ³æ•°ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            concurrent_limit: ä¸¦åˆ—å®Ÿè¡Œæ•°ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            initial_question: åˆæœŸè³ªå•ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

        Returns:
            pd.DataFrame: ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼çµæœ
        """
        return asyncio.run(
            self.run_async(
                input_file=input_file,
                output_file=output_file,
                max_turns=max_turns,
                concurrent_limit=concurrent_limit,
                initial_question=initial_question,
            )
        )
